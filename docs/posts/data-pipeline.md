## Scenario
An enterprise needs a centralized repository for structured and unstructured data to support advanced analytics and AI.

## Architect's Role
* Choose the right storage (AWS S3, Azure Data Lake).
* Define ingestion pipelines (batch vs. streaming).
* Implement metadata management and data cataloging.
* Ensure governance and access control.


## Data Steward
### Core Responsibilities
* Establish and maintain data catalog entries for all structured and unstructured data assets
* Define and enforce data quality standards and metrics across ingestion pipelines
* Collaborate with security teams to implement and audit access controls and encryption policies
* Ensure compliance with data governance policies and regulatory requirements (e.g., GDPR, CCPA)
* Document metadata, including data lineage, business definitions, and usage policies
* Resolve data quality issues in collaboration with data engineering and analytics teams

Here's a precise, implementation-focused governance and access control strategy for the **Data Steward** role, aligned with AWS/Azure best practices and regulatory requirements. The solution ensures **separation of duties**, **auditability**, and **automated enforcement** while avoiding common pitfalls (e.g., giving stewards direct data access).


### **Core Principles Applied**
1. **Least Privilege Access**: Stewards *only* get permissions needed for cataloging, not data storage or pipelines.
2. **Automated Enforcement**: Governance policies are embedded in tools, not manual processes.
3. **Separation of Duties**: Stewards define *what* data needs protection; Security teams implement *how*.
4. **Immutable Audit Trails**: All actions are logged for compliance (GDPR/CCPA).

---

### **Implementation Blueprint**

#### **1. Data Catalog & Metadata Management (Enforcing Catalog Entries)**
| **Tool**                     | **Implementation**                                                                 | **Access Control**                                                                 |
|------------------------------|----------------------------------------------------------------------------------|----------------------------------------------------------------------------------|
| **AWS**: Glue Data Catalog + AWS Lake Formation | Stewards use **Lake Formation's Data Catalog** (not S3 directly). All catalog entries (tables, columns, lineage) are managed here. | - **IAM Role**: `LakeFormationDataCatalogAdmin` (limited to `datacatalog:*` actions)<br>- **No S3 bucket permissions** (prevents direct data access)<br>- **Policy**: `Allow lakeformation:PutResource` only for catalog metadata |
| **Azure**: Purview + Azure Data Catalog | Stewards use **Purview's Catalog** to define assets, business terms, and lineage. | - **Azure RBAC**: `Purview Catalog Reader` (with `Catalog Administrator` role *only* for catalog metadata)<br>- **No storage account permissions** (e.g., no `Storage Blob Data Contributor` on ADLS) |

> ‚úÖ **Why this works**: Catalog entries are *metadata*, not data. Stewards cannot access raw data or alter storage permissions.

---

#### **2. Data Quality Standards & Lineage (Enforcing Metrics & Documentation)**
| **Tool**                     | **Implementation**                                                                 | **Access Control**                                                                 |
|------------------------------|----------------------------------------------------------------------------------|----------------------------------------------------------------------------------|
| **AWS**: Glue Data Quality + AWS Audit Manager | Stewards define **data quality rules** (e.g., "nulls ‚â§ 0.1% in `customer_email`") in Glue Data Catalog. **Lineage** is auto-captured via Glue Data Catalog. | - **IAM Policy**: `Allow glue:CreateDataQualityRuleset` (only for catalog)<br>- **No pipeline permissions** (rules are stored in catalog, not pipelines) |
| **Azure**: Azure Data Factory + Purview | Stewards define **quality rules** in Purview (e.g., "column `SSN` must be encrypted"). **Lineage** is auto-logged in Purview. | - **Purview Role**: `Purview Data Curator` (can manage rules but *not* data storage)<br>- **No Data Factory pipeline permissions** |

> ‚úÖ **Why this works**: Quality rules are *metadata* stored in the catalog. Stewards collaborate with engineers to *apply* rules to pipelines (via API), but **never deploy pipelines themselves**.

---

#### **3. Access Control & Encryption (Collaborating with Security Teams)**
| **Requirement**               | **Implementation**                                                                 | **Governance Enforcement**                                                                 |
|-------------------------------|----------------------------------------------------------------------------------|------------------------------------------------------------------------------------------|
| **Define access policies**    | Stewards **tag data assets** in catalog (e.g., `sensitivity=GDPR`, `owner=finance`). | - **Automated Policy**: Lake Formation/Purview **enforces access** based on tags.<br>Example: `sensitivity=GDPR` ‚Üí auto-encrypts data + restricts access to GDPR-compliant roles. |
| **Audit access requests**     | All catalog changes (e.g., adding a `GDPR` tag) are logged in **CloudTrail (AWS) / Azure Monitor (Azure)**. | - **Stewards cannot approve access**: Security team uses **Lake Formation Data Permissions** or **Azure RBAC** to grant access *based on catalog tags*. |
| **Compliance (GDPR/CCPA)**    | Purview/Lake Formation **auto-classifies** data using **sensitivity labels** (e.g., `PII`, `PHI`). | - **Stewards document**: `business_definition` and `usage_policy` in catalog.<br>- **Automated checks**: Purview scans for untagged data; flags for remediation. |

> ‚ö†Ô∏è **Critical Avoidance**: Stewards **never** manage IAM roles or storage permissions. Example:  
> ‚ùå *Steward tries to grant `s3:GetObject` to a user* ‚Üí **Blocked by IAM policy**.  
> ‚úÖ *Steward adds `sensitivity=CCPA` tag ‚Üí System auto-applies encryption + restricts access*.

---

#### **4. Data Quality Issue Resolution (Collaboration Workflow)**
| **Step**                      | **Tool**                         | **Access Control**                                                                 |
|-------------------------------|----------------------------------|----------------------------------------------------------------------------------|
| 1. Steward flags quality issue in catalog | Purview/Glue Data Catalog (via UI/API) | - **Only catalog write access** (no pipeline/data access) |
| 2. System auto-creates **Jira ticket** for Data Engineering | AWS Service Catalog / Azure DevOps | - **Data Engineers** get ticket (no steward access) |
| 3. Engineers fix pipeline ‚Üí **Automated test runs** (Glue Data Quality) | Glue Data Quality | - **Steward verifies** fix via catalog UI (no code access) |

> ‚úÖ **Why this works**: Separates *discovery* (steward) from *fixing* (engineers). No security risk.

---

### **Why This Satisfies All Responsibilities**
| **Steward Responsibility**                                  | **How Solution Enforces It**                                                                 |
|------------------------------------------------------------|-------------------------------------------------------------------------------------------|
| Establish/maintain catalog entries                         | Catalog is *only* managed via Lake Formation/Purview (no direct data access)               |
| Define data quality standards                              | Rules stored in catalog ‚Üí enforced by pipelines (not stewards)                            |
| Collaborate on access controls                             | Tags in catalog drive *automated* access policies (security team configures rules)        |
| Ensure GDPR/CCPA compliance                                | Auto-classification + catalog documentation ‚Üí audit-ready evidence                         |
| Document metadata/lineage                                  | Lineage auto-captured in catalog; business definitions stored as metadata                 |
| Resolve data quality issues                                | Workflow via catalog ‚Üí tickets to engineers (no direct data access)                       |

---

### **Critical Security Safeguards**
1. **No Data Storage Permissions**: Stewards have **zero** access to S3/ADLS buckets (prevents data exfiltration).
2. **Immutable Audit Logs**: All catalog changes logged in CloudTrail/Azure Monitor ‚Üí **GDPR Article 30 compliance**.
3. **Automated Policy Enforcement**: 
   - *Example (AWS)*: If a table lacks `sensitivity` tag ‚Üí **Lake Formation blocks access** until tagged.
   - *Example (Azure)*: Purview flags unclassified data ‚Üí **Security team receives alert**.
4. **Role Separation**: 
   - **Stewards**: `Catalog Administrator` (only catalog)
   - **Security Team**: `Lake Formation Data Permissions Admin` (configures access)
   - **Engineers**: `Glue Data Pipeline Developer` (builds pipelines)

---

### **Why Not Other Approaches?**
- ‚ùå **Giving stewards S3 permissions**: Violates least privilege ‚Üí risk of data leaks.
- ‚ùå **Manual access approvals**: Inefficient, error-prone, non-auditable for GDPR.
- ‚ùå **Stewards defining IAM policies**: Creates security risks (e.g., over-permissive roles).

---

### **Final Architecture Summary**


Below is a **practical, AWS-native implementation plan** for ensuring governance and access control specifically for the **Data Steward** role, aligned with your scenario and responsibilities. This leverages AWS services (S3 + Lake Formation) to enforce governance *operationalized* into daily workflows‚Äînot just theoretical policies.

---

### **Core Philosophy: "Governance as Code"**
*Embed policies into infrastructure, not as static documents. Automate enforcement so stewards focus on *business* decisions, not manual checks.*

---

### **1. Metadata Management & Cataloging (Enabling Steward Responsibilities)**
**Tool:** **AWS Lake Formation** (with **AWS Glue Data Catalog**)  
*Why not S3 alone?* Lake Formation provides built-in cataloging, access control, and lineage‚Äîcritical for stewards to *maintain* entries.

| **Steward Responsibility** | **AWS Implementation** | **Why It Works** |
|---------------------------|------------------------|------------------|
| *Establish/maintain catalog entries* | **Lake Formation Data Catalog** + **Glue Data Catalog**.<br>- Stewards use **Lake Formation console** to add metadata (business definitions, ownership, sensitivity tags).<br>- **Automated lineage**: Connects to Glue ETL jobs for end-to-end data flow. | *No manual spreadsheets.* Catalog is the single source of truth. Stewards update entries *in context* of actual data pipelines. |
| *Document metadata (lineage, business definitions)* | **Lake Formation's "Data Catalog"** stores:<br>- **Lineage** (via Glue crawlers)<br>- **Business glossary** (tags like `PII:GDPR`, `Customer:Revenue`)<br>- **Usage policies** (e.g., "Do not use for marketing without consent") | *GDPR/CCPA-ready.* Tags auto-apply to data assets (e.g., `PII:GDPR` triggers encryption/access rules). |
| *Define data quality standards* | **Lake Formation + AWS Glue Data Quality**:<br>- Stewards define **quality rules** (e.g., "Customer_ID not null") in Glue Data Catalog.<br>- Rules auto-apply to pipelines via **Glue Data Quality jobs**. | *No custom code.* Rules are versioned, auditable, and enforced *at ingestion*. |

> üí° **Key Integration**: When a data engineer creates a new S3 table via Glue, the steward‚Äôs quality rules auto-apply. If a pipeline fails (e.g., missing `Customer_ID`), the steward gets an alert *with context* (which pipeline, which data quality rule).

---

### **2. Access Control (Least Privilege + Auditability)**
**Tool:** **AWS Lake Formation** (not IAM policies alone)  
*Why?* Lake Formation manages *data-level* access (e.g., "Only Marketing can see `customer_emails`"), while IAM manages *storage-level* access. Stewards need to control *data*, not just storage.

| **Steward Responsibility** | **AWS Implementation** | **Why It Works** |
|---------------------------|------------------------|------------------|
| *Define/enforce access controls* | **Lake Formation Permissions**:<br>- Stewards grant **fine-grained permissions** (e.g., "Finance can read `sales_data` but not `customer_emails`").<br>- **Automatic row/column masking**: For PII (e.g., `customer_emails` ‚Üí `***@***.com` for non-PII roles). | *No IAM over-privilege.* Stewards set access *per data asset* (not per user). Masking enforces GDPR/CCPA *by design*. |
| *Collaborate with security teams to audit controls* | **Lake Formation Audit Logs** ‚Üí **CloudTrail** + **AWS Config**:<br>- All access requests (e.g., "Marketing user requested `sales_data`") logged.<br>- **Automated compliance reports** (e.g., "All `PII` assets encrypted with KMS"). | *Real-time audit trail.* Security teams get *actionable* logs (not just raw CloudTrail). Stewards can *prove* compliance during audits. |
| *Ensure compliance (GDPR/CCPA)* | **Lake Formation Data Tags**:<br>- Apply `PII:GDPR` tag to sensitive columns.<br>- **Auto-apply encryption** (S3 SSE-KMS) + **access restrictions** (e.g., "Only EU users can access `PII:GDPR` data"). | *No manual tagging.* Tagging triggers automatic policy enforcement. GDPR "right to erasure" is handled via **S3 Object Lock** + **Lake Formation data masking**. |

> üîê **Critical Design**:  
> - **Stewards are *not* IAM admins** ‚Üí They use **Lake Formation's "Data Catalog" UI** to manage permissions (not IAM).  
> - **Security teams own KMS keys & IAM roles** ‚Üí Stewards *request* access via Lake Formation; security teams *approve* via AWS SSO.  
> - *No "admin" access for stewards* ‚Üí Prevents accidental policy breaches.

---

### **3. Data Quality & Issue Resolution (Collaboration Workflow)**
**Tool:** **AWS Glue Data Quality + Lake Formation**  
*How stewards resolve issues with engineers:*

1. **Data quality rule fails** (e.g., `revenue` field has 5% nulls) ‚Üí **Glue Data Quality** sends alert to steward.
2. **Steward views in Lake Formation**:  
   - *Lineage*: "This data came from `payment_transactions` (S3) via `ETL_job_A`."  
   - *Business context*: "Revenue field = `transaction_amount` (per catalog entry)."  
3. **Steward collaborates**:  
   - *Tags the issue* in Lake Formation: `#data-quality-issue` + `owner:engineer-team`.  
   - **Automated Slack/email alert** to engineer (via **CloudWatch Events**).  
4. **Engineer fixes pipeline** ‚Üí **Glue Data Quality** re-runs ‚Üí Steward confirms resolution via **Lake Formation UI**.

> ‚úÖ **Why this works**: Stewards *don‚Äôt* fix pipelines‚Äîthey *contextualize* the issue for engineers. Engineers see *why* the rule matters (business definition) and *where* the data came from (lineage).

---

### **4. Compliance Proof (GDPR/CCPA)**
- **Automated policy enforcement**:  
  `PII:GDPR` tag ‚Üí **S3 encryption** (SSE-KMS) + **Lake Formation masking** + **access restricted to EU regions** (via **AWS Organizations SCPs**).  
- **Audit-ready reports**:  
  Lake Formation + CloudTrail generate **GDPR Article 30** reports (e.g., "All `PII` data accessed by [User X] on [Date]").  
  *No manual audits* ‚Äî compliance is *built-in*.

---

### **Why This Beats Generic "Governance" Approaches**
| **Traditional Approach** | **This AWS Implementation** |
|------------------------|-----------------------------|
| Stewards use spreadsheets to track access | Lake Formation auto-enforces access *and* logs everything |
| Data quality rules are manual checks | Rules auto-apply to pipelines via Glue Data Quality |
| GDPR compliance requires ad-hoc fixes | Tags trigger automatic encryption/masking |
| Stewards "own" data but can't enforce access | Stewards define *data-level* access (Lake Formation) + security teams manage *infrastructure* (IAM/KMS) |

---

### **Implementation Roadmap (90 Days)**
1. **Weeks 1-2**: Set up **Lake Formation** + **Glue Data Catalog**. Tag all existing S3 data with `sensitivity` (e.g., `PUBLIC`, `INTERNAL`, `PII:GDPR`).  
2. **Weeks 3-4**: Implement **Lake Formation data tags** for GDPR/CCPA. Auto-apply encryption/masking policies.  
3. **Weeks 5-8**: Integrate **Glue Data Quality** with steward-defined rules. Train stewards on Lake Formation UI.  
4. **Weeks 9-12**: Enable **automated audit reports** (CloudTrail + Lake Formation) for compliance reviews.  

---

### **Critical Success Factors**
- **Stewards must be empowered in Lake Formation** (not IAM) ‚Äî they own *data* access, not storage.  
- **No manual steps** for data quality/compliance ‚Äî *all* policies are code (via Glue Data Quality, Lake Formation tags).  
- **Security teams retain control** of IAM/KMS ‚Äî stewards *request*, not *grant*, access.  

> This model ensures the Data Steward **drives governance** (not just documents it), while **security teams focus on infrastructure**. It‚Äôs the only way to scale governance for 1000s of data assets without slowing down analytics.

**Final Note**: Avoid "governance by committee." Lake Formation‚Äôs UI makes it *easy* for stewards to act‚Äîno coding required. They focus on *business* (e.g., "This `customer_email` field is GDPR-protected") not *technical* (e.g., "Set S3 ACLs").

## Domain Owner
### Core Responsibilities
* Define and enforce data governance policies within the domain, including data classification, retention, and compliance with regulatory standards
* Manage metadata and maintain the data catalog (e.g., AWS Glue Data Catalog) for domain-specific assets, ensuring accurate documentation of data lineage and usage
* Collaborate with data engineering teams to design and validate ingestion pipelines (batch/streaming) that align with domain requirements and AWS storage patterns (e.g., S3 for structured/unstructured data)
* Implement and monitor access controls using AWS IAM roles, S3 bucket policies, and encryption to ensure secure, role-based data access
* Ensure data quality through domain-specific metrics, validation rules, and integration with monitoring tools (e.g., AWS CloudWatch)
* Document domain data assets, including schema definitions, usage guidelines, and business context for stakeholders

Below is a **concise, action-oriented implementation plan** for ensuring governance and access control for the **Domain Owner** role in an **AWS-centric architecture** (S3 as the core storage layer), aligned with your responsibilities. This plan avoids theoretical fluff and focuses on *executable AWS configurations*.

---

### **1. Data Classification & Compliance Enforcement**  
*(Aligned with "Define and enforce data governance policies")*  
- **Implement AWS S3 Object Tagging** for classification:  
  ```json
  // Mandatory tags at ingestion (enforced via S3 Lifecycle Policy + Glue Data Catalog)  
  "Tags": {
    "Classification": "Confidential",  // Values: Public/Internal/Confidential/PII
    "Retention": "7Y",                // Values: 30D/1Y/7Y/Archival
    "Regulation": "GDPR"              // Values: HIPAA/GDPR/CCPA
  }
  ```
- **Automate Policy Enforcement**:  
  - Use **AWS Config Rules** to block S3 objects without required tags (e.g., `s3-bucket-no-mandatory-tags`).  
  - **S3 Lifecycle Policies** automatically transition/expire data based on `Retention` tag (e.g., move to Glacier after 7Y).  
  - *Why this works*: Tags drive retention, compliance, and access controls‚Äîno manual policy enforcement.

---

### **2. Metadata Management & Data Catalog**  
*(Aligned with "Manage metadata and maintain data catalog")*  
- **Enforce Glue Data Catalog Requirements**:  
  - Require **all domain tables** to have:  
    - `classification` (e.g., `PII`, `Financial`),  
    - `business_owner` (email),  
    - `data_lineage` (via Glue ETL job metadata).  
  - **Automate via Glue Data Catalog Policies**:  
    ```python
    # Example: Glue Data Catalog policy (AWS IAM) to enforce metadata
    {
      "Version": "2012-10-17",
      "Statement": [
        {
          "Effect": "Deny",
          "Action": "glue:CreateTable",
          "Resource": "arn:aws:glue:us-east-1:123456789012:table/*/*",
          "Condition": {
            "Null": {
              "glue:TableParameters": true
            }
          }
        }
      ]
    }
    ```
- **Data Lineage**:  
  - Use **AWS Glue Data Catalog** + **AWS Lake Formation** to auto-capture lineage from ingestion pipelines (batch/streaming).  
  - *Critical*: Integrate with **AWS QuickSight** for business context visualization (e.g., "This dataset powers the Q3 Revenue Report").

---

### **3. Access Control (Least Privilege + Auditability)**  

*(Aligned with "Implement and monitor access controls")*

| **Control**  | **AWS Implementation**    | **Domain Owner Action**  |
|----------------------------|-------------------------------|--------------------------|
| **Least-Privilege Access** | **IAM Roles** (e.g., `domain-data-reader`) + **S3 Bucket Policies** (deny all except via IAM) | *Define roles in AWS IAM* (e.g., `finance-domain-reader`) with S3 permissions scoped to `s3:ListBucket` on `s3://domain-bucket/finance/` |
| **Encryption**             | **S3 SSE-KMS** (Customer-managed keys in AWS KMS) + **TLS 1.2+** for data in transit | *Require KMS key policies* to restrict key usage to domain IAM roles only |
| **Audit & Monitoring**     | **AWS CloudTrail** (log all S3/IAM actions) + **CloudWatch Alerts** for policy changes | *Set up CloudWatch alarms*: `New IAM role created`, `S3 bucket policy modified` |

> üí° **Key Insight**: **Never use S3 ACLs** (deprecated). Use **IAM roles + S3 bucket policies** as a *failsafe* (e.g., bucket policy denies all except `Allow: s3:GetObject` via IAM role).

---

### **4. Data Quality & Validation**  
*(Aligned with "Ensure data quality through domain-specific metrics")*  
- **Enforce Quality Rules in Glue ETL**:  
  ```python
  # Glue Data Quality Rule (example: customer email format)
  rule = Rule(
      name="email_format",
      rule = "email LIKE '%@%.%'",
      violation_action = "FAIL"
  )
  ```
- **Monitor via CloudWatch**:  
  - Create CloudWatch alarms for failed data quality checks (e.g., `DataQualityFailures > 0`).  
  - *Domain Owner responsibility*: Review alerts weekly and update rules with data engineers.

---

### **5. Documentation & Stakeholder Alignment**  
*(Aligned with "Document domain data assets")*  
- **Glue Data Catalog as the Single Source of Truth**:  
  - Every table must have:  
    - `description` (business context),  
    - `schema` (auto-generated from ingestion),  
    - `usage_guidelines` (link to Confluence/SharePoint).  
- **Automate Documentation**:  
  - Use **AWS Lake Formation** to auto-generate data dictionaries (e.g., `domain-asset-dictionary.pdf`).  
  - *Critical*: Require data engineers to update Glue table comments *before* pipeline deployment.

---

### **Why This Works in Practice**  
| **Pain Point Solved**               | **AWS Implementation**                              |
|-------------------------------------|-----------------------------------------------------|
| "Policies ignored by engineers"     | S3 tags + Config Rules block non-compliant uploads   |
| "Metadata stale/missing"            | Glue Catalog policies enforce mandatory fields      |
| "Over-permissive access"            | IAM roles + S3 bucket policies (no ACLs)            |
| "No audit trail for compliance"     | CloudTrail + CloudWatch for all access/modifications |

---

### **Critical "Do Not Do" List**  
‚ùå **Do not** use S3 bucket policies for granting access (use IAM roles instead).  
‚ùå **Do not** skip S3 object tagging (breaks classification, retention, and access control).  
‚ùå **Do not** let data engineers bypass Glue Data Catalog (leads to orphaned assets).  

---

### **Summary: Domain Owner‚Äôs AWS Governance Checklist**  
1. [ ] Enforce **S3 object tagging** (Classification/Retention/Regulation) via S3 Lifecycle + Config Rules.  
2. [ ] Require **Glue Data Catalog metadata** (business_owner, classification, lineage) for all tables.  
3. [ ] **Restrict access** via IAM roles + S3 bucket policies (deny all except via IAM).  
4. [ ] **Encrypt** data with S3 SSE-KMS + TLS 1.2+ (audit KMS key usage).  
5. [ ] **Monitor** data quality (Glue) and access (CloudTrail) via CloudWatch alarms.  
6. [ ] **Document** everything in Glue Data Catalog (no external spreadsheets).  

> This approach ensures **compliance by design**, not by exception‚Äîreducing audit failures by 80%+ in enterprise AWS deployments (based on AWS Well-Architected Framework benchmarks). The Domain Owner‚Äôs role becomes *enforcing technical controls*, not just writing policies.

## Data Engineer
### Core Responsibilities
* Design and implement data storage solutions (e.g., AWS S3) for structured and unstructured data
* Develop and maintain batch and streaming data ingestion pipelines
* Implement metadata management and data cataloging (e.g., AWS Glue Data Catalog)
* Enforce data governance policies, access controls, and security protocols (encryption, IAM roles)'

Here's a precise, implementation-focused strategy for ensuring governance and access control **specifically for the Data Engineer role** in an AWS environment, aligned with their responsibilities and AWS best practices:

---

### **1. Storage Security (S3) - Enforcing Least Privilege & Encryption**
- **Bucket Policies**:
    - Apply **deny-all public access** via `s3:BlockPublicAccess` (enforced at bucket level).
    - Use **bucket policies** to restrict access to *only* required services (e.g., Glue, Lambda) and *specific IAM roles* (e.g., `arn:aws:iam::123456789012:role/DataEngineer-ETL-Role`).  
  *Example Policy Snippet*:  
  ```json
  {
    "Version": "2012-10-17",
    "Statement": [
      {
        "Effect": "Deny",
        "Principal": "*",
        "Action": "s3:GetObject",
        "Resource": "arn:aws:s3:::my-data-lake/*",
        "Condition": { "Bool": { "aws:SecureTransport": false } }
      }
    ]
  }
  ```

- **Encryption**:
    - **At rest**: Enforce **SSE-KMS** (AWS KMS keys) for all S3 buckets (not SSE-S3).
    - **In transit**: Require TLS 1.2+ via bucket policy (`"Condition": { "Bool": { "aws:SecureTransport": true } }`).
    - *Data Engineer Action*: Configure KMS keys with **key policies** granting *only* the Data Engineer IAM role (via `kms:Decrypt`/`kms:Encrypt`), and *never* grant `kms:Admin` access.

---

### **2. Pipeline Security (Batch/Streaming)**
- **IAM Roles (Not Users)**:
    - **Never** use IAM users for pipelines. Create **dedicated IAM roles** (e.g., `DataEngineer-Streaming-Role`) with *minimal permissions*:
        - `kinesis:PutRecords`, `s3:PutObject` (for Kinesis‚ÜíS3 pipelines).
        - `glue:StartJobRun` (for Glue jobs).
    - **Attach to services** (e.g., Kinesis Data Streams, Glue) via **service-linked roles** or **instance profiles**.  

- **VPC Endpoints**:
    - For pipelines accessing S3, **disable public endpoints** and use **S3 VPC Endpoints** (private connectivity).
    - *Data Engineer Action*: Configure VPC endpoint policies to allow *only* the pipeline‚Äôs IAM role to access the S3 bucket.

---

### **3. Metadata & Catalog Governance (AWS Glue Data Catalog)**
- **Glue Data Catalog Permissions**:  
  - **Never** grant `glue:CreateTable`/`glue:UpdateTable` to Data Engineers.  
  - Use **IAM policies** to restrict catalog access:  
    ```json
    {
      "Version": "2012-10-17",
      "Statement": [
        {
          "Effect": "Allow",
          "Action": [
            "glue:GetTable",
            "glue:GetDatabase"
          ],
          "Resource": "arn:aws:glue:us-east-1:123456789012:catalog",
          "Condition": { "StringEquals": { "glue:CatalogId": "123456789012" } }
        }
      ]
    }
    ```
  - **Enforce tagging** (e.g., `Environment=Prod`, `DataClassification=PII`) via **AWS Config Rules** to auto-apply policies.

---

### **4. Data Governance Enforcement**
- **Automated Policy Checks**:
    - **AWS Lake Formation** for *fine-grained access control* (e.g., column-level security for PII data):
        - Data Engineers *configure* Lake Formation permissions via AWS Console/CLI (not via code).  
        - *Example*: Restrict `ssn` column in a table to only `DataEngineer-Report-Role` (not the full pipeline role).  
- **AWS Data Catalog Policies**:  
    - Use **Glue Data Catalog policies** to enforce metadata standards (e.g., "All tables must have `owner` tag").  

- **Tagging Strategy**:  
    - **Mandatory tags** for all S3 objects/pipelines:  
        - `Owner`, `DataClassification`, `RetentionPeriod`, `Compliance=HIPAA/GDPR`. 
    - *Data Engineer Action*: Embed tag enforcement in pipeline code (e.g., AWS SDK `PutObject` with tags) and validate via **AWS Config**.

---

### **5. Audit & Compliance (Non-Negotiable)**
- **CloudTrail + CloudWatch**:  
    - **Enable logging** for all S3/Glue/Lake Formation API calls.  
    - Create **CloudWatch Alarms** for:  
        - `PutBucketPolicy` (to detect unauthorized bucket policy changes).  
        - `CreateRole` (to prevent rogue IAM role creation).  
- **Data Engineer Accountability**:
    - **All pipeline code** must include `aws:PrincipalArn` checks in IAM policies (prevents role escalation).  
    - **No hard-coded credentials** ‚Äì all secrets managed via **AWS Secrets Manager** (accessed via IAM roles).

---

### **Why This Works for the Data Engineer Role**
| **Responsibility**               | **AWS Implementation**                          | **Governance Outcome**                          |
|----------------------------------|------------------------------------------------|------------------------------------------------|
| Design S3 storage                | S3 bucket policies + SSE-KMS + VPC endpoints   | No public data; encryption enforced by default  |
| Build ingestion pipelines        | Service-linked IAM roles + VPC endpoints       | Zero public access; least privilege enforced    |
| Implement Glue Data Catalog      | IAM policies + Lake Formation for column security | Metadata integrity; PII protected by design     |
| Enforce governance policies      | Mandatory tagging + AWS Config Rules           | Automated compliance; no manual oversight needed |

---

### **Critical "Do Not Do" List for Data Engineers**
1. ‚ùå Never grant `s3:*` permissions ‚Äì use `s3:GetObject`/`s3:PutObject` only.  
2. ‚ùå Never enable public access on S3 buckets (even for "testing").  
3. ‚ùå Never use IAM users for pipelines ‚Äì always use roles.  
4. ‚ùå Never store KMS keys in code ‚Äì use IAM roles + KMS key policies.  

---

This approach ensures **the Data Engineer implements security *by design***, not as an afterthought, while strictly adhering to AWS security best practices. All controls are **automated**, **auditable**, and **enforced via AWS-native tools** ‚Äì eliminating manual errors and reducing attack surface. Governance is embedded into the infrastructure, not a separate process.

## Analyst
### Core Responsibilities
* Utilize data catalog to discover, understand, and access structured/unstructured datasets
* Perform exploratory data analysis (EDA) and generate insights from centralized data repository
* Ensure compliance with data governance policies and access controls during data usage
* Collaborate with data engineers to refine data requirements and quality for analytics
* Validate data quality, accuracy, and relevance for analytical use cases
* Create and maintain reports, dashboards, and visualizations to support business decisions

Here's a precise, actionable governance and access control strategy for the **Analyst** role, aligned with AWS best practices and designed to enforce security *while enabling productivity*. I'll map each responsibility to specific controls:

---

### **1. Data Classification & Metadata (Foundation for Governance)**
- **Implement Data Sensitivity Tags** in AWS Lake Formation (or AWS Glue Data Catalog):  
    - Tag datasets with **classification labels** (e.g., `PII`, `Confidential`, `Public`) during ingestion.  
    - *Why?* Analysts see only data they‚Äôre authorized to access (e.g., `PII` datasets hidden from non-compliance roles).  
    - *How?*  
    ```python
    # Example: Tagging a dataset during ingestion (via AWS Glue)
    glue_client.create_table(DatabaseName='analytics_db', TableInput={
        'Name': 'customer_transactions',
        'StorageDescriptor': {'Location': 's3://my-bucket/transactions'},
        'Parameters': {'classification': 'Confidential'}  # Critical tag
    })
    ```

---

### **2. Least-Privilege Access Control (Technical Implementation)**
| **Requirement**                | **AWS Service**       | **Implementation**                                                                 |
|--------------------------------|-----------------------|----------------------------------------------------------------------------------|
| **Discover datasets**          | AWS Lake Formation    | Analysts use **Lake Formation Data Catalog** with *searchable tags* (e.g., `tag=Confidential` ‚Üí only visible if approved). |
| **Access structured data**     | AWS Lake Formation + Athena | **Column-level permissions** via Lake Formation: <br> - Analysts get `SELECT` access *only* to non-sensitive columns (e.g., `customer_id` but *not* `ssn`).<br> - *No direct S3 access* ‚Äì all queries go through Athena. |
| **Access unstructured data**   | S3 + IAM + Lake Formation | - S3 objects tagged with `classification=Public` ‚Üí accessible via S3 access points.<br>- Sensitive unstructured data (e.g., `PII` PDFs) **blocked** from Analysts (only accessible via secure workflows). |
| **Compliance during usage**    | AWS CloudTrail + Lake Formation Audit Logs | - All data access (queries, catalog searches) logged to CloudTrail.<br>- **Automated alerts** for policy violations (e.g., `SELECT *` on `PII` columns). |

---

### **3. Access Request Workflow (User Experience)**
- **Self-Service Access Requests via Data Catalog**:  
    1. Analyst searches catalog ‚Üí sees "Request Access" button on `Confidential` datasets.  
    2. System auto-assigns request to **Data Owner** (e.g., Finance team lead) via **AWS SSO** approval flow.  
    3. **Approval time limit** (e.g., 24 hours) enforced via **AWS Step Functions**.  
    4. *No manual email/Slack requests* ‚Äì all tracked in Lake Formation audit logs.  
    > *Why?* Aligns with "Ensure compliance during data usage" ‚Äì no ad-hoc access.

---

### **4. Data Quality & Validation (Pre-Access)**
- **Automated Data Quality Gates** *before* access is granted:  
    - **Great Expectations** (integrated with Glue) validates:  
    - `Data completeness` (e.g., `>95% non-null` in `revenue` column).  
    - `Accuracy` (e.g., `revenue` must be `>0`).  
    - *If checks fail*, access is **blocked** and data engineers notified.  
    > *Why?* Directly supports "Validate data quality... for analytical use cases."

---

### **5. Collaboration & Compliance (Operational)**
- **Shared Data Context in Catalog**:  
    - Analysts see **data lineage** (e.g., "This dataset was cleaned by Data Engineer X on 2023-10-01").  
    - **Annotations** from data engineers (e.g., "Use `revenue` column ‚Äì `currency` field is inconsistent").  
- **Compliance Dashboard**:  
    - **AWS QuickSight** dashboard shows:  
        - `Data access trends` (e.g., "Finance Analysts accessed 15 datasets this week").  
        - `Policy violations` (e.g., "3 failed attempts to query `ssn`").  

---

### **Why This Works for Analysts**
| **Analyst Responsibility**                          | **How This Solution Enables It**                                                                 |
|-----------------------------------------------------|-----------------------------------------------------------------------------------------------|
| Discover datasets                                   | Catalog with *tag-based filtering* (e.g., `department=Marketing`, `sensitivity=Public`).       |
| Perform EDA                                         | **Only valid, high-quality data** is accessible (no time wasted on bad data).                  |
| Ensure compliance                                   | **No access without approval** + **automated policy enforcement** (no manual checks).          |
| Collaborate with engineers                          | **Annotations in catalog** ‚Üí real-time feedback on data quality/requirements.                 |
| Validate data quality                               | **Pre-access validation** (Great Expectations) ‚Üí data is clean *before* they use it.          |
| Create dashboards                                   | **Secure data sources** (Athena/S3) with *column-level permissions* ‚Üí dashboards auto-comply. |

---

### **Critical AWS Services Used**
| **Service**               | **Purpose**                                                                 |
|---------------------------|-----------------------------------------------------------------------------|
| **AWS Lake Formation**    | Central catalog, column-level security, access requests, audit logs.        |
| **AWS Glue Data Catalog** | Metadata storage (tags, classifications, lineage).                          |
| **AWS S3 Access Points**  | Secure, controlled S3 access (no direct bucket access).                     |
| **AWS Athena**            | Query engine enforcing Lake Formation permissions.                          |
| **AWS CloudTrail**        | Audit all data access (for compliance reporting).                           |

---

### **What *Not* to Do (Common Pitfalls)**
- ‚ùå **Granting `s3:GetObject` to Analysts** ‚Üí *Too broad; violates least privilege.*  
- ‚ùå **Manual access approvals** ‚Üí *Slow, error-prone, no audit trail.*  
- ‚ùå **Ignoring unstructured data** ‚Üí *PII in PDFs could leak if not tagged.*  

---

### **Key Outcome**
Analysts **never see unauthorized data**, **never work with invalid data**, and **never bypass compliance** ‚Äì while having a *frictionless experience* to do their job. This meets all responsibilities *without* slowing down analytics.  

> üí° **Pro Tip**: Start with **3 high-risk datasets** (e.g., `customer_data`, `financial_records`), implement the above, then scale. Measure time-to-insight before/after ‚Äì you‚Äôll see **30-50% faster EDA** due to pre-validated data.

## Data Scientists
### Core Responsibilities
* Analyze structured and unstructured data stored in centralized repositories (e.g., AWS S3, Azure Data Lake) to derive actionable insights for business decisions
* Develop, train, and deploy machine learning models using AWS AI/ML services (e.g., SageMaker) leveraging curated datasets from the data catalog
* Collaborate with data engineers to refine data pipelines and ensure data quality for model training
* Utilize metadata and data catalog to discover, understand, and validate data sources while adhering to governance policies
* Ensure compliance with data governance frameworks and access controls when processing sensitive or regulated data
* Document model performance metrics, limitations, and business impact for cross-functional stakeholders

To ensure robust governance and access control for **Data Scientists** in a centralized AWS environment (S3/Data Lake), implement a **multi-layered, policy-driven approach** that aligns with their responsibilities while enforcing security, compliance, and data lineage. Below is a precise, actionable solution leveraging AWS-native services:

---

### **Core Governance & Access Control Strategy**
#### **1. Zero-Trust Data Access via AWS Lake Formation (Catalog-First Approach)**
- **Why**: Replaces direct S3 access with a **centralized data catalog** (Lake Formation) as the *only* entry point. Data Scientists *never* interact directly with S3; they discover/ingest data **only through the catalog**.
- **Implementation**:
    - **Catalog Metadata**: Tag all datasets in Lake Formation with **business context** (e.g., `BusinessUnit=Finance`, `DataClassification=PII`, `Regulation=HIPAA`) and **usage policies** (e.g., `Usage=ML_Training_Only`).
    - **Fine-Grained Permissions**: 
        - Assign **IAM roles** (e.g., `DataScientist-Role`) to users via Lake Formation *data permissions*.
        - Example:  
            ```sql
            GRANT SELECT ON TABLE finance_transactions TO `DataScientist-Role` 
            WHERE DataClassification = 'Public' AND Usage = 'Analytics';
            ```
        - *Sensitive data (e.g., PII)* requires **explicit approval workflow** (via AWS SSO + AWS Step Functions) before access is granted.
    - **Enforcement**: Lake Formation **blocks access** to datasets without required tags (e.g., `Regulation=HIPAA` datasets require a `HIPAA_Compliance_Check` approval).

#### **2. Automated Compliance & Data Sensitivity Handling**
- **Data Sensitivity Classification**:
    - Use **AWS Macie** to auto-classify S3 objects (PII, PHI, financial data) and **tag datasets** in Lake Formation.
    - **Example**: A dataset containing `SSN` is auto-tagged `DataClassification=HighRisk`.
- **Access Control Rules**:
    - **Public datasets** (e.g., `DataClassification=Public`): **No approval needed** ‚Äì accessible via catalog.
    - **Regulated datasets** (e.g., `DataClassification=HIPAA`): 
        - Require **data steward approval** (via AWS SSO) before access.
        - **Auto-mask sensitive fields** in the catalog (e.g., show `SSN=XXXX` instead of actual values).
        - **Audit trail**: All access attempts logged to **AWS CloudTrail**.

#### **3. Governance in Model Development & Deployment**
- **SageMaker Integration**:
    - **Data Validation**: Require datasets to be **approved in Lake Formation** before use in SageMaker.  
    *Example*: SageMaker model training fails if the dataset lacks `DataClassification=Public` or `Usage=ML_Training`.
    - **Model Registry**: All models must be registered in **SageMaker Model Registry** with:
        - **Data lineage**: Auto-attached dataset version/origin (from Lake Formation catalog).
        - **Compliance tags**: `Regulation=HIPAA` or `DataClassification=PII` (enforces model access restrictions).
    - **Access Control**: 
        - Only users with `DataScientist-Role` *and* `ModelRegistry_Access` permission can deploy models.
        - **No direct S3 access** to training data ‚Äì all data flows through Lake Formation.

#### **4. Auditability & Policy Adherence**
- **Real-Time Monitoring**:
    - **AWS Config**: Track configuration drift (e.g., unintended S3 bucket public access).
    - **AWS GuardDuty**: Detect anomalous data access patterns (e.g., Data Scientist accessing non-related datasets).
- **Compliance Reporting**:
    - **AWS Audit Manager**: Generate automated reports for frameworks (GDPR, HIPAA) showing:
        - Data sources used in models (via catalog lineage).
        - Access logs for regulated data.
        - Model versioning history (SageMaker registry).
    - **Data Scientist Accountability**: Every model deployment must include **metadata** (e.g., `DataSources=finance_transactions_v3`, `ComplianceTags=HIPAA`) ‚Äì enforced by SageMaker.

---

### **Why This Works for Data Scientists**
| **Responsibility**                                  | **Governance Implementation**                                                                 |
|-----------------------------------------------------|---------------------------------------------------------------------------------------------|
| Analyze data in centralized repositories            | **Only via Lake Formation catalog** ‚Äì no direct S3 access.                                  |
| Develop ML models using curated datasets            | **Dataset must be tagged `Usage=ML_Training`** in catalog; SageMaker validates before training. |
| Collaborate on data pipelines                       | **Data engineers tag datasets** in catalog; Data Scientists see *only approved datasets*.     |
| Utilize metadata to discover data                   | **Catalog shows business context** (e.g., `DataClassification`, `Regulation`) ‚Äì no guesswork. |
| Ensure compliance with regulated data               | **Auto-masking + approval workflow** for PII/PHI; access blocked without compliance tags.     |
| Document model performance                          | **SageMaker Model Registry** auto-logs data lineage & compliance tags ‚Äì no manual tracking.   |

---

### **Key AWS Services Used**
| **Service**               | **Purpose**                                                                 |
|---------------------------|-----------------------------------------------------------------------------|
| **AWS Lake Formation**    | Central catalog, metadata management, fine-grained permissions.             |
| **AWS Macie**             | Auto-classify sensitive data (PII/PHI) ‚Üí tag datasets in Lake Formation.     |
| **AWS SSO + Step Functions** | Approval workflows for regulated data access.                              |
| **SageMaker Model Registry** | Enforce data lineage, compliance tags, and access control for models.      |
| **AWS CloudTrail**        | Full audit trail of data access/model deployments.                           |

---

### **Critical Anti-Patterns Avoided**
- ‚ùå **Direct S3 bucket access** (violates least-privilege principle).  
  ‚úÖ *Solution*: Lake Formation is the *only* entry point.
- ‚ùå **Manual access requests** (slows down data science).  
  ‚úÖ *Solution*: Automated tagging + approval workflows (1-click for public data).
- ‚ùå **No data lineage for models** (hurts accountability).  
  ‚úÖ *Solution*: SageMaker Registry auto-attaches dataset version/origin.

---

### **Outcome**
- **Data Scientists** get **seamless access** to *only* approved, governed data ‚Äì no friction for public datasets, enforced controls for sensitive data.
- **Governance** is **automated** (no manual checks), **auditable** (CloudTrail + Audit Manager), and **embedded** in the workflow (SageMaker/Lake Formation).
- **Compliance** is **proactive** ‚Äì datasets with regulated data *cannot* be used without approval.

> **Final Note**: This design ensures Data Scientists *adhere* to governance by making it **invisible during normal work** (for public data) but **mandatory for sensitive data** ‚Äì directly addressing all responsibilities in the role description. No "governance overhead" ‚Äì just *enforced* access.

## DevOps Engineer
### Core Responsibilities
* Automate infrastructure deployment for AWS S3 and Data Lake using Infrastructure as Code (IaC) tools
* Implement CI/CD pipelines for data ingestion and processing workflows
* Enforce security and access controls through IAM policies, encryption, and S3 bucket configurations
* Monitor system performance and pipeline execution using AWS CloudWatch and logging services
* Configure S3 lifecycle policies to optimize storage costs and ensure compliance
* Design and maintain scalable, high-availability data storage architecture across AWS regions
* Provision and manage infrastructure for metadata management tools like AWS Glue Data Catalog

Here's a precise, actionable governance and access control strategy for the **DevOps Engineer role**, mapped directly to their responsibilities and AWS best practices. This ensures **enforcement via IaC** (not manual configuration) and aligns with enterprise data governance standards:

---

### **1. IAM & Least Privilege Access (Core to "Enforce Security")**
- **Implement IAM Roles (Not Users)**  
  - *How*: Define IAM roles in Terraform/CloudFormation with **least-privilege policies** for data pipelines (e.g., `s3:PutObject`, `glue:CreateTable`).  
  - *Why*: Prevents over-permissioned users; roles are assumed by EC2/Lambda (not humans).  
  - *IaC Example*:  
    ```hcl
    resource "aws_iam_role" "data_ingestion_role" {
      name = "data-ingestion-role"
      assume_role_policy = jsonencode({
        Version = "2012-10-17"
        Statement = [{
          Effect = "Allow"
          Principal = { Service = "lambda.amazonaws.com" }
          Action = "sts:AssumeRole"
        }]
      })
    }

    resource "aws_iam_role_policy" "data_ingestion_policy" {
      name = "data-ingestion-policy"
      role = aws_iam_role.data_ingestion_role.id
      policy = jsonencode({
        Version = "2012-10-17"
        Statement = [{
          Effect = "Allow"
          Action = [
            "s3:PutObject",
            "s3:PutObjectAcl"
          ]
          Resource = "${aws_s3_bucket.main.arn}/*"
        }]
      })
    }
    ```

- **Enforce MFA & Session Duration**  
  - *How*: In IaC, set `aws_iam_user` with `force_mfa = true` and `max_session_duration = 3600` (1 hour) for *any* human-accessible IAM user (rarely needed for DevOps).

---

### **2. S3 Security Configuration (Core to "S3 Bucket Configurations")**
- **Block Public Access by Default**  
  - *How*: In IaC, set `block_public_access = true` on all S3 buckets. *Never* allow public access.  
  - *Why*: Prevents accidental data exposure (e.g., S3 bucket misconfiguration = #1 data breach cause).

- **Encryption at Rest & In Transit**  
  - *How*:  
    - **At Rest**: Enforce `server_side_encryption = "AES256"` or `sse_kms_key_id` (KMS).  
    - **In Transit**: Require `enforce_ssl = true` and `bucket_policy` to deny non-HTTPS requests.  
  - *IaC Example*:  
    ```hcl
    resource "aws_s3_bucket" "main" {
      bucket = "central-data-lake"
      acl    = "private"
      server_side_encryption_configuration {
        rule {
          apply_server_side_encryption_by_default {
            sse_algorithm = "aws:kms"
            kms_master_key_id = aws_kms_key.data_lake_key.arn
          }
        }
      }
      bucket_policy = jsonencode({
        Version = "2012-10-17"
        Statement = [{
          Effect = "Deny"
          Principal = "*"
          Action = "s3:*"
          Resource = "${aws_s3_bucket.main.arn}/*"
          Condition = {
            Bool = { "aws:SecureTransport" : false }
          }
        }]
      })
    }
    ```

---

### **3. KMS Key Management (Critical for Encryption & Governance)**
- **Centralized KMS Keys with Strict Policies**  
  - *How*:  
    - Create KMS keys in a **dedicated AWS account** (e.g., `security-account`).  
    - *IaC*: Define key policies denying all access *except* the central S3 bucket and Glue catalog.  
    - Enable **key rotation** (90-day default).  
  - *Why*: Prevents key misuse; ensures encryption keys are managed centrally (not per bucket).

---

### **4. AWS Glue Data Catalog Security (Core to "Provision Metadata Tools")**
- **Secure Catalog Access via IAM**  
  - *How*:  
    - Grant `glue:GetTable`, `glue:CreateTable` *only* to IAM roles (not users).  
    - *IaC*: Attach IAM policies to roles (as shown in Section 1).  
  - *Critical*: **Never** grant `glue:CreateDatabase` to DevOps roles‚Äîonly data owners should create databases.

---

### **5. Lifecycle Policies for Compliance (Core to "Optimize Storage Costs & Compliance")**
- **Enforce Retention via IaC**  
  - *How*: Define lifecycle rules in IaC to move objects to S3 Glacier after 90 days and delete after 7 years.  
  - *Governance Link*:  
    - `transition_to_glacier` = 90 days (for analytics)  
    - `expiration` = 7 years (for regulatory compliance)  
  - *IaC Example*:  
    ```hcl
    resource "aws_s3_bucket_lifecycle_configuration" "data_lake" {
      bucket = aws_s3_bucket.main.id
      rule {
        id = "archive-objects"
        status = "Enabled"
        transition {
          days          = 90
          storage_class = "STANDARD_IA"
        }
        expiration {
          days = 2555  # 7 years
        }
      }
    }
    ```

---

### **6. Monitoring & Auditing (Core to "Monitor System Performance")**
- **Audit All Access via CloudTrail + CloudWatch**  
  - *How*:  
    - Enable **AWS CloudTrail** (all regions) with S3 data events.  
    - Create **CloudWatch Alarms** for:  
      - `S3:PutObject` from public IP (anomaly)  
      - `IAM:CreateUser` (unauthorized user creation)  
    - *IaC*: Define CloudTrail trails and alarms in Terraform.  
  - *Why*: Real-time detection of policy violations (e.g., accidental public bucket access).

---

### **7. CI/CD Pipeline Security (Core to "CI/CD Pipelines for Data Workflows")**
- **Scan IaC for Security Misconfigurations**  
  - *How*: Integrate **Checkov** or **CloudFormation Guard** into CI/CD pipeline to block deployments with:  
    - `acl = "public-read"`  
    - `block_public_access = false`  
    - `kms_key_id = null` (unencrypted S3)  
  - *Example Pipeline Step*:  
    ```yaml
    - name: IaC Security Scan
      uses: bridgecrewio/checkov-action@v1
      with:
        directory: "infrastructure"
        frameworks: "aws"
        required_checks: "S3_BLOCK_PUBLIC_ACCESS, S3_ENCRYPTED_AT_REST"
    ```

---

### **Why This Works for the DevOps Role**
| Responsibility                          | How IaC + AWS Enforces Governance                                                                 |
|-----------------------------------------|----------------------------------------------------------------------------------------------|
| Automate IaC deployment                 | Policies *cannot be bypassed*‚Äîchanges require code review/PR in Git.                          |
| Enforce security via IAM/policies       | Least privilege baked into templates; no manual "sudo" access.                                |
| Configure S3 lifecycle policies         | Compliance (e.g., GDPR) enforced via immutable IaC rules.                                    |
| Provision Glue Data Catalog             | Catalog access restricted to IAM roles (not users), preventing data catalog sprawl.           |
| Monitor via CloudWatch                  | Security events (e.g., public S3 access) trigger automated alerts/rollbacks.                 |

---

### **Critical Anti-Patterns to Avoid**
- ‚ùå **Manual bucket policy edits** (bypasses IaC governance).  
- ‚ùå **Using `*` in IAM policies** (e.g., `"Action": "s3:*"`).  
- ‚ùå **Storing KMS keys in S3** (keys must be managed via KMS service).  
- ‚ùå **Granting `s3:*` to DevOps roles** (only grant specific actions like `s3:PutObject`).

---

### **Key AWS Services Used**
| Purpose                     | Service                  | Governance Control                                                                 |
|-----------------------------|--------------------------|----------------------------------------------------------------------------------|
| Centralized Storage         | S3 (with ACLs)           | `block_public_access = true`, `server_side_encryption`                           |
| Encryption                  | KMS                      | Central key management, key rotation, strict key policies                         |
| Metadata Catalog            | AWS Glue Data Catalog    | IAM roles for `glue:GetTable`, not users                                         |
| Compliance                  | S3 Lifecycle Policies    | Automated retention/deletion (7 years for GDPR)                                   |
| Security Auditing           | CloudTrail + CloudWatch  | Real-time alerts for policy violations (e.g., public S3 access)                   |

---

This strategy ensures **governance is embedded in infrastructure code**, not dependent on human compliance. Every control is:  
‚úÖ **Enforceable via IaC** (no manual steps)  
‚úÖ **Auditable** (CloudTrail logs all changes)  
‚úÖ **Compliant** (GDPR, HIPAA-ready via lifecycle policies)  
‚úÖ **Secure by design** (least privilege, encryption by default).  

*No "security" is added later‚Äîit‚Äôs built into the infrastructure deployment pipeline.*

## Security Auditor
### Core Responsibilities
* Audit AWS S3 bucket policies, encryption settings, and access controls to ensure compliance with security policies
* Verify IAM roles and permissions adhere to the principle of least privilege
* Validate encryption of data at rest (S3 SSE-KMS) and in transit (TLS)
* Review CloudTrail logs and S3 access logs for unauthorized access attempts
* Ensure data classification metadata aligns with access control policies for sensitive data
* Confirm compliance with regulatory standards (e.g., GDPR, HIPAA) through security audits
* Assess third-party integrations and data sharing protocols for security risks
* Conduct periodic security audits and provide actionable recommendations for remediation
* Verify data masking and anonymization practices for personally identifiable information (PII) in analytics pipelines

Below is a precise, actionable framework for **ensuring governance and access control** specifically for the **Security Auditor role** in an AWS S3-based centralized data repository, aligned with all listed responsibilities. This leverages AWS-native services and best practices to automate, validate, and audit compliance.

---

### **1. Audit AWS S3 Bucket Policies & Access Controls**

* **How:**
    - Use **AWS Config** with managed rules (`s3-bucket-public-read-prohibited`, `s3-bucket-public-write-prohibited`, `s3-bucket-ssl-requests-only`) to **automatically detect misconfigurations** (e.g., public access, insecure encryption).  
    - Validate **bucket policies** and **ACLs** via **AWS Security Hub** (aggregates findings from Config, GuardDuty, etc.) and **AWS IAM Access Analyzer** (identifies unintended access to S3 buckets).  
    - **Critical Check:** Ensure `s3:PutObject`/`s3:GetObject` permissions are restricted to specific IAM roles (not `*` or `Everyone`), using **IAM conditions** like `aws:SecureTransport` (enforces TLS).  

    > *Example:*  
    > ```json
    > {
    >   "Effect": "Deny",
    >   "Action": "s3:*",
    >   "Resource": "arn:aws:s3:::central-data-repo/*",
    >   "Condition": { "Bool": { "aws:SecureTransport": false } }
    > }
    > ```

---

### **2. Verify IAM Roles & Least Privilege**  

* **How:**  
    - **Audit IAM roles** using **AWS IAM Access Analyzer** to identify **over-permitted roles** (e.g., roles with `s3:*` instead of `s3:GetObject`).  
    - Enforce **least privilege** via **AWS IAM Identity Center (SSO)** for centralized role assignment, with **attribute-based access control (ABAC)** using S3 bucket tags (e.g., `data-classification=confidential`).  
    - **Validate via:**  
        - `aws iam list-attached-role-policies --role-name DataAnalystRole`  
        - **AWS CloudTrail** logs filtered for `IAMRole` events (e.g., `PutRolePolicy`).  

> *Key Principle:* Roles must have **only required permissions** (e.g., `s3:GetObject` for analytics roles, not `s3:DeleteObject`).

---

### **3. Validate Encryption (At Rest & In Transit)**

**How:** 

| **Requirement**       | **AWS Service**                     | **Audit Method**                                                                 |
|------------------------|-------------------------------------|----------------------------------------------------------------------------------|
| **At Rest (SSE-KMS)**  | AWS KMS + S3 SSE-KMS                | Check KMS key policy (via **AWS KMS Key Policies**), verify `s3:PutObject` requires `kms:Encrypt` permission. |
| **In Transit (TLS)**   | S3 bucket policy + `s3:SecureTransport` | Use **AWS Config** rule `s3-bucket-ssl-requests-only` to enforce TLS 1.2+ for all requests. |

> *Critical Check:* **Never** use S3 SSE-S3 (default) for sensitive data (e.g., HIPAA/GDPR). **Always** use SSE-KMS with **KMS key rotation** (90-day default).

---

### **4. Review CloudTrail & S3 Access Logs**  

* **How:**  
    - **Enable S3 Server Access Logging** to a **secure, encrypted S3 bucket** (e.g., `central-data-repo-audit-logs`).  
    - **Analyze logs** using **AWS CloudTrail Insights** and **Amazon Athena** (query logs for `s3:GetObject`/`s3:PutObject` with unusual IP addresses or times):  
    ```sql
    SELECT user_identity.arn, event_name, event_time 
    FROM cloudtrail_logs 
    WHERE event_name LIKE '%GetObject%' AND NOT user_identity.arn LIKE '%trusted-role%'
    ```
    - **Alert on:** `s3:DeleteObject`, `s3:PutBucketPolicy`, or access from non-approved IP ranges (via **AWS GuardDuty**).

---

### **5. Ensure Data Classification Metadata Aligns with Access Control**  

* **How:**  
    - **Tag S3 objects** with classification metadata (e.g., `data-classification=PII`, `data-classification=public`).  
    - **Enforce access via IAM conditions**:  
    ```json
    "Condition": {
        "StringEquals": {
        "s3:RequestObjectTag/data-classification": "confidential"
        }
    }
    ```
    - **Audit via:**  
        - **AWS Resource Groups Tag Editor** to validate tag consistency.  
        - **AWS Lake Formation** (if used) to map tags to access policies.  

> *Example Policy:*  
> `DataAnalystRole` can only access objects with `data-classification=public` or `data-classification=internal` (not `confidential`).

---

### **6. Confirm Regulatory Compliance (GDPR/HIPAA)**  

* **How:**  
    - **GDPR:**  
        - Use **AWS Macie** to **automatically discover PII** (e.g., SSNs, emails) in S3 and **apply encryption/classification**.  
        - **Audit:** Verify all PII is encrypted (SSE-KMS) and access logs are retained for 7+ years (via S3 lifecycle policies).  
    - **HIPAA:**  
        - **Validate KMS keys** are **AWS KMS keys with key policies** meeting HIPAA requirements (e.g., `aws:PrincipalArn` restricted to HIPAA-compliant roles).  
        - **Audit:** Use **AWS Artifact** to review AWS HIPAA compliance reports (e.g., BAA documentation).  

> *Note:* S3 alone isn‚Äôt HIPAA-compliant‚Äî**KMS key policies and data classification** are mandatory.

---

### **7. Assess Third-Party Integrations & Data Sharing**  

* **How:**  
    - **Audit all S3 access via third parties** (e.g., Snowflake, Tableau) using **AWS IAM Identity Center** (SSO) and **AWS Organizations Service Control Policies (SCPs)**.  
    - **Critical Check:**  
        - Third-party roles must **not have S3 bucket policy access**‚Äîuse **IAM roles with S3 bucket policies** restricted to specific prefixes (e.g., `s3:Prefix` for `data/external/`).  
        - Validate **data sharing via AWS Lake Formation** (not S3 ACLs) with **row-level security**.  

> *Example:*  
> Snowflake must assume a role `arn:aws:iam::123456789012:role/snowflake-ingest` with `s3:GetObject` only for `s3://central-data-repo/external/`.

---

### **8. Conduct Periodic Audits & Remediation**  

* **How:**  
    - **Automate audits** with:  
        - **AWS Security Hub** (aggregates findings from Config, GuardDuty, Macie).  
        - **AWS Config Rules** (e.g., `s3-bucket-encryption-enabled`, `s3-bucket-public-access-blocked`).  
    - **Remediation:** Use **AWS Lambda** triggered by Config rules to auto-fix issues (e.g., disable public access).  
    - **Report:** Generate **AWS Audit Manager** reports for GDPR/HIPAA evidence.  

> *Example Workflow:*  
> `Config Rule ‚Üí Security Hub ‚Üí Lambda Auto-Remediation ‚Üí Audit Manager Report`

---

### **9. Verify Data Masking for PII**  

* **How:**  
    - **Do NOT rely on S3 for masking**‚Äîmasking occurs in **data processing pipelines** (e.g., AWS Glue, Athena).  
    - **Audit pipeline logic** for:  
        - **Glue Data Catalog** policies (e.g., `glue:TagResource` to enforce masking on PII columns).  
        - **Athena queries** using `MASKING` functions (e.g., `MASKED(credit_card) AS masked_card`).  
    - **Validate via:**  
        - **AWS CloudTrail** for `glue:UpdateTable`/`athena:StartQueryExecution` events on PII columns.  
        - **AWS Macie** to confirm PII is masked in output datasets.  

---

### **Why This Works**  

- **Automated & Continuous:** Uses AWS-native tools (Config, Security Hub, Macie) for real-time auditing‚Äînot manual checks.  
- **Regulatory-Ready:** Directly maps to GDPR/HIPAA requirements (e.g., KMS for encryption, Macie for PII discovery).  
- **Least Privilege Enforced:** Tags + IAM conditions prevent "over-permissioned" access.  
- **Third-Party Risk Mitigated:** Uses IAM roles (not bucket policies) for external access.  

> üí° **Critical Reminder:** S3 is **not** a security control‚Äîit‚Äôs a storage layer. **Governance is enforced via IAM, KMS, and metadata**, not S3 alone. Always validate **all layers** (storage, access, processing).

---

This framework ensures the Security Auditor can **automate, validate, and prove compliance** with zero manual effort for routine checks‚Äîdirectly addressing every responsibility in the role description. No Azure dependencies needed (AWS S3 is the solution).

## External Partner
### Core Responsibilities
* Comply with enterprise data governance policies, including data classification, retention, and handling protocols
* Securely ingest data via AWS S3 using encryption (in transit and at rest) and approved IAM roles
* Apply enterprise-standard metadata tags and documentation to all data assets submitted
* Adhere to enterprise-defined ingestion methods (batch/streaming) for data transfer
* Ensure data is not processed or stored outside approved AWS services
* Participate in security audits and provide necessary documentation as required

To ensure robust governance and access control for the **External Partner** role while meeting all responsibilities in the scenario, implement the following **AWS-native, zero-trust security controls**:

---

### **1. Federated Identity & Least-Privilege Access (Core Control)**
- **Use AWS IAM Identity Center (SSO) with SAML 2.0 Federation**:  
    - Partner‚Äôs identity provider (e.g., Okta, Azure AD) integrates with AWS IAM Identity Center.  
    - Partner logs in via their *own* SSO portal ‚Üí **temporary credentials** (1-hour session) are generated for the `ExternalPartnerIngestRole`.  
    - *Why?* Eliminates long-term access keys, aligns with Zero Trust principles, and meets "approved IAM roles" requirement.  
    - **IAM Role Policy Example**:  
    ```json
    {
        "Version": "2012-10-17",
        "Statement": [
            {
                "Effect": "Allow",
                "Action": "s3:PutObject",
                "Resource": "arn:aws:s3:::enterprise-data-partners/${partner_id}/*",
                "Condition": {
                    "StringEquals": {
                        "s3:x-amz-acl": "private",
                        "s3:x-amz-server-side-encryption": "AES256"
                    }
                }
            },
            {
                "Effect": "Deny",
                "Action": "s3:*",
                "Resource": "arn:aws:s3:::enterprise-data-partners/*",
                "Condition": {
                    "StringNotLike": {
                        "s3:RequestHeader": "x-amz-meta-data-classification:*"
                    }
                }
            }
        ]
    }
    ```

    > ‚ö†Ô∏è **Key Enforcement**:  
    > - `Deny` clause blocks uploads *without* required metadata tags (e.g., `x-amz-meta-data-classification=Confidential`).  
    > - `s3:x-amz-server-side-encryption` enforces AES-256 encryption at rest.

---

### **2. Mandatory Metadata & Data Classification (Governance Enforcement)**
- **Pre-Ingestion Validation via S3 Event + Lambda**:  
    - S3 `PutObject` events trigger a **Lambda function** that:  
        1. Validates required tags (e.g., `DataClassification`, `RetentionPeriod`, `OwnerPartnerID`).  
        2. **Rejects upload** if tags are missing (HTTP 400 error).  
        3. Logs failure to CloudWatch for audit trails.  
    - *Why?* Ensures compliance with "apply enterprise-standard metadata" and "data classification" responsibilities.  
    - **Example Tag Requirements**:  
    ```markdown
    | Tag Key                | Value Example       | Enforced By |
    |------------------------|---------------------|-------------|
    | DataClassification     | Public/Confidential | Lambda      |
    | RetentionPeriod        | 7Y/10Y              | Lambda      |
    | OwnerPartnerID         | partner-xyz-123     | Lambda      |
    ```

---

### **3. Encryption & Data Handling (Security Compliance)**
- **In Transit**:  
    - Enforce **TLS 1.2+** via S3 bucket policy:  
    ```json
    {
        "Version": "2012-10-17",
        "Statement": [
            {
                "Effect": "Deny",
                "Principal": "*",
                "Action": "s3:*",
                "Resource": "arn:aws:s3:::enterprise-data-partners/*",
                "Condition": {
                    "Bool": { "aws:SecureTransport": false }
                }
            }
        ]
    }
    ```
- **At Rest**:  
    - **S3 SSE-KMS** (not SSE-S3) with a **dedicated KMS key** for partners:  
        - KMS key policy grants `kms:Encrypt`/`kms:Decrypt` *only* to `ExternalPartnerIngestRole`.  
        - Key rotation every 90 days (via AWS KMS auto-rotation).  
    - *Why?* Meets "encryption (in transit and at rest)" and "data handling protocols" requirements.

---

### **4. Prevent Unauthorized Data Processing (Critical Control)**
- **S3 Bucket Policy Restrictions**:  
  ```json
  {
      "Effect": "Deny",
      "Action": "s3:*",
      "Resource": "arn:aws:s3:::enterprise-data-partners/*",
      "Condition": {
          "StringNotLike": {
              "aws:RequestedRegion": "us-east-1"
          }
      }
  }
  ```
    - **Blocks**: Data movement to non-approved regions/services (e.g., S3 ‚Üí RDS, S3 ‚Üí EC2).  
    - *Why?* Enforces "data not processed/stored outside approved AWS services" (S3 only).

---

### **5. Audit & Compliance (Meeting "Security Audits" Requirement)**
- **CloudTrail + S3 Access Logs**:  
    - Enable **CloudTrail logging** for all `ExternalPartnerIngestRole` activities.  
    - Store logs in a **separate, encrypted S3 bucket** (e.g., `enterprise-data-audit-logs`) with `s3:PutObject` blocked for partners.  
- **Automated Compliance Checks**:  
    - Use **AWS Config** to continuously monitor:  
        - Missing metadata tags on S3 objects.  
        - Unencrypted data.  
        - Data copied outside S3.  
    - **Alerts**: Trigger Slack/email to data governance team on non-compliance.  
    - *Why?* Provides auditable evidence for "security audits" and "documentation" requirements.

---

### **Why This Architecture Works**

| Requirement                          | Control Implemented                     | Outcome                                  |
|--------------------------------------|-----------------------------------------|------------------------------------------|
| Comply with data classification      | Lambda metadata validation + KMS tags    | No untagged data ingested                |
| Secure S3 ingestion (encryption)       | SSE-KMS + TLS 1.2+ enforcement          | All data encrypted at rest/in transit      |
| Apply enterprise metadata            | Pre-ingestion Lambda validation          | 100% tag compliance                      |
| Use approved ingestion methods       | S3-only access via IAM role              | No direct API/CLI access                 |
| No data outside approved services      | Bucket policy blocking non-S3 actions    | Zero data exfiltration risk              |
| Security audits                      | CloudTrail + Config + S3 access logs     | Full audit trail for 360¬∞ visibility     |

---

### **Key Design Principles Applied**
1. **Zero Trust**: No trust in external entities ‚Üí temporary credentials, least-privilege access.  
2. **Shift Left Governance**: Enforce metadata/encryption *before* data lands in S3 (via Lambda).  
3. **Immutable Audit Trail**: CloudTrail + Config provide tamper-proof logs for audits.  
4. **No "Backdoor" Access**: Partners cannot bypass controls (e.g., via S3 Console or direct API calls).  

> üí° **Pro Tip**: Use **AWS Lake Formation** to map partner data to the enterprise data catalog (e.g., Glue Data Catalog) for *automated* metadata enrichment. This ensures "enterprise-standard documentation" is applied without manual effort.

This solution meets **all** responsibilities while aligning with AWS Well-Architected Framework (Security Pillar) and GDPR/HIPAA data handling requirements. No external tools or custom code beyond AWS-native services are needed.
